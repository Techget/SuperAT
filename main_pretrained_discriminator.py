import torch
import torch.nn as nn
import torchvision
from torchvision.utils import make_grid
import torchvision.transforms as transforms
import numpy as np
import matplotlib.pyplot as plt
import torch
from torch.autograd import Variable
from dataloader import dataloader
import utils
import os
from pytorch_lightning.utilities.seed import seed_everything
from vae import VanillaVAE
from discriminator import DiscriminatorRes

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
torch.autograd.set_detect_anomaly(True)


# hyper parameter
# convert to use arg parser
epochs=100
lr=3e-4
alpha=0.1
gamma=15
is_adversarial_training = False
batch_size = 32
kld_weight = 0.001 # for VAE (Used in standara libraries)
gen_discrim_loss_weight = 0.1 # for the loss of discriminator with input generated by generator
                          # to make sure these errors are in the same numeric level


# logging and saving result
loss_log_interval = 20
image_log_interval = 200
sample_size = 16
checkpoint_dir='./train_vae_with_pretrained_discriminator/checkpoints'


# For reproducibility
seed_everything(7, True)


data_loader=dataloader(batch_size)
gen=VanillaVAE().to(device)
discrim=DiscriminatorRes().to(device)

# load pretrained resnet 18
state_dict = os.path.join(
  "state_dicts", "resnet18" + ".pt"
)
discrim.load_state_dict(torch.load(state_dict))

criterion = nn.CrossEntropyLoss().to(device)
optim_gen=torch.optim.RMSprop(gen.parameters(), lr=lr)
optim_Dis=torch.optim.RMSprop(discrim.parameters(), lr=lr*alpha)

for epoch in range(epochs):
  for i, (data, targets) in enumerate(data_loader, 0):
    datav = Variable(data).to(device)
    targets = Variable(targets).to(device)
    x_reconstructed, _, mean, logvar = gen(datav)

    reconstruction_loss = gen.reconstruction_loss(x_reconstructed, datav)
    kl_divergence_loss = gen.kl_divergence_loss(mean, logvar) * kld_weight
    vae_loss = reconstruction_loss + kl_divergence_loss

    discrim_with_generated_input = discrim(x_reconstructed)
    gen_output_to_discrim_CE_error = criterion(torch.tensor(discrim_with_generated_input), targets) * gen_discrim_loss_weight

    gen_loss = vae_loss - gen_output_to_discrim_CE_error

    gen_loss.backward()
    optim_gen.step()

    if is_adversarial_training:
      discrim_output_real_image = discrim(datav)
      disrim_real_image_CE = criterion(discrim_output_real_image, targets)
      discrim_loss = disrim_real_image_CE + gen_output_to_discrim_CE_error
      discrim_loss.backward()
      optim_Dis.step()

    # print out losses status
    if i % loss_log_interval  == 0:
      print('[%d/%d][%d/%d]\treconstruction_loss: %.4f\tkl_divergence_loss: %.4f\tgen_output_to_discrim_CE_error: %.4f\tgen_loss: %0.4f'
        % (epoch,epochs, i, len(data_loader),
          reconstruction_loss.item(), 
          kl_divergence_loss.item(),
          gen_output_to_discrim_CE_error.item(),
          gen_loss.item()))
      print()
      if is_adversarial_training:
        print('econstruction_loss: %.4f\tkl_divergence_loss: %.4f' 
          % (disrim_real_image_CE.item(), discrim_loss.item()))

    # if i % image_log_interval == 0:
    #   images = gen.sample(datav, device)
    #   plt.imshow(np.squeeze(images))

  utils.save_checkpoint(gen, checkpoint_dir+'./vae_with_pretrained_discriminator', epoch)
  if is_adversarial_training:
    utils.save_checkpoint(discrim, checkpoint_dir, epoch)

 